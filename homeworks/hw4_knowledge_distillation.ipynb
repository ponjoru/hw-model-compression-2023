{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1Nf4H3tIx6ZlD4t_Y3IeWydpA7aK3zx71","authorship_tag":"ABX9TyOewrX8Uk07zrZbvKOwLVwf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"mi0xC-_hZbWL","executionInfo":{"status":"ok","timestamp":1698052076917,"user_tz":-180,"elapsed":11070,"user":{"displayName":"Igor Popov","userId":"17510105061651099427"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"abcfdea6-2b42-4915-f673-9b5ca5e1cb85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: loguru in /usr/local/lib/python3.10/dist-packages (0.7.2)\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.10/dist-packages (1.2.0)\n","Requirement already satisfied: albumentations_experimental in /usr/local/lib/python3.10/dist-packages (0.0.1)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (1.23.5)\n","Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (2.1.0+cu118)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from torchmetrics) (0.9.0)\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.10/dist-packages (from albumentations_experimental) (1.3.1)\n","Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (23.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.5.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.12.4)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.1->torchmetrics) (2.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from albumentations->albumentations_experimental) (1.11.3)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.10/dist-packages (from albumentations->albumentations_experimental) (0.19.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from albumentations->albumentations_experimental) (6.0.1)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from albumentations->albumentations_experimental) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from albumentations->albumentations_experimental) (4.8.1.78)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.10/dist-packages (from qudida>=0.0.4->albumentations->albumentations_experimental) (1.2.2)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->albumentations_experimental) (9.4.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->albumentations_experimental) (2.31.5)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->albumentations_experimental) (2023.9.26)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.16.1->albumentations->albumentations_experimental) (1.4.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.1->torchmetrics) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.1->torchmetrics) (1.3.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->albumentations_experimental) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations->albumentations_experimental) (3.2.0)\n"]}],"source":["!pip install loguru torchmetrics albumentations_experimental"]},{"cell_type":"code","source":["from torchvision.datasets.widerface import WIDERFace\n","WIDERFace(root='data', split='train', download=True)"],"metadata":{"id":"SlkQJRVdZk4I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698049254380,"user_tz":-180,"elapsed":84974,"user":{"displayName":"Igor Popov","userId":"17510105061651099427"}},"outputId":"d22a78f9-7720-4b44-fa04-76007c282920"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["1465602149it [00:10, 145850180.47it/s]\n","362752168it [00:08, 41324991.34it/s]\n","1844140520it [00:14, 128914963.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Downloading http://shuoyang1213.me/WIDERFACE/support/bbx_annotation/wider_face_split.zip to data/widerface/wider_face_split.zip\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 3591642/3591642 [00:01<00:00, 1909871.30it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Extracting data/widerface/wider_face_split.zip to data/widerface\n"]},{"output_type":"execute_result","data":{"text/plain":["Dataset WIDERFace\n","    Number of datapoints: 12880\n","    Root location: data/widerface\n","    Split: train"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["import itertools\n","import cv2\n","import torch\n","import numpy as np\n","from pathlib import Path\n","from torch.utils.data import Dataset\n","\n","\n","def xyxy2xywh(x):\n","    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n","    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n","    y[:, 0] = x[:, 0]\n","    y[:, 1] = x[:, 1]\n","    y[:, 2] = x[:, 2] - x[:, 0]  # width\n","    y[:, 3] = x[:, 3] - x[:, 1]  # height\n","    return y\n","\n","\n","def xywh2xyxy(x):\n","    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n","    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n","    y[:, 0] = x[:, 0]\n","    y[:, 1] = x[:, 1]\n","    y[:, 2] = x[:, 0] + x[:, 2]  # bottom right x\n","    y[:, 3] = x[:, 1] + x[:, 3]  # bottom right y\n","    return y\n","\n","\n","def simple_collate_fn(batch):\n","    img, bb, kp, img_id, img_path, img_shape = zip(*batch)  # transposed\n","    for i, b in enumerate(bb):\n","        b[:, 0] = i  # add target image index\n","    for i, k in enumerate(kp):\n","        k[:, 0] = i  # add target image index\n","\n","    new_batch = {\n","        'image': torch.stack(img, 0),\n","        # targets\n","        'bb': torch.cat(bb, 0),\n","        'kp': torch.cat(kp, 0),\n","        # meta\n","        'img_id': img_id,\n","        'img_path': img_path,\n","        'img_shape': img_shape,\n","    }\n","\n","    return new_batch\n","\n","\n","class SimpleCustomBatch:\n","    def __init__(self, data):\n","        img, bb, kp, img_id, img_path, img_shape = list(zip(*data))\n","        for i, b in enumerate(bb):\n","            b[:, 0] = i  # add target image index\n","        for i, k in enumerate(kp):\n","            k[:, 0] = i  # add target image index\n","\n","        self.image = torch.stack(img, 0)\n","        self.bb = torch.cat(bb, 0)\n","        self.kp = torch.cat(kp, 0)\n","        self.img_path = img_path\n","        self.img_id = img_id\n","        self.img_shape = img_shape\n","\n","    # custom memory pinning method on custom type\n","    def pin_memory(self):\n","        self.image = self.image.pin_memory()\n","        self.bb = self.bb.pin_memory()\n","        self.kp = self.kp.pin_memory()\n","        return self\n","\n","    def __getitem__(self, item):\n","        return getattr(self, item)\n","\n","\n","def collate_fn(batch):\n","    return SimpleCustomBatch(batch)\n","\n","\n","def is_in_image(point, shape):\n","    return 0 <= point[0] < shape[0] and 0 <= point[1] < shape[1]\n","\n","\n","class WiderFaceDataset(Dataset):\n","    NK = 5\n","    BB_CLASS_LABELS = ('Face', )\n","    KP_CLASS_LABELS = ['left_eye', 'right_eye', 'nose', 'left_mouth', 'right_mouth']\n","\n","    def __init__(self, ds_path, mode, min_size=None, transforms=None, color_layout='RGB'):\n","        super(WiderFaceDataset, self).__init__()\n","        self.min_size = min_size\n","        self.ds_path = ds_path\n","        self.mode = mode\n","        self.transforms = transforms\n","        self.color_layout = color_layout\n","\n","        self.gt_path = str(Path(ds_path) / f'WIDER_{self.mode}' / 'labelv2.txt')\n","\n","        self.bb_cat2id = {cat: idx for idx, cat in enumerate(self.BB_CLASS_LABELS)}\n","        self.bb_id2cat = {idx: cat for idx, cat in enumerate(self.BB_CLASS_LABELS)}\n","\n","        self.kp_cat2id = {cat: idx for idx, cat in enumerate(self.KP_CLASS_LABELS)}\n","        self.kp_id2cat = {idx: cat for idx, cat in enumerate(self.KP_CLASS_LABELS)}\n","\n","        self.images, self.annotations = self.load_annotations(self.gt_path)\n","\n","    def __len__(self):\n","        return len(self.annotations)\n","\n","    def _load_image(self, idx):\n","        info = self.images[idx]\n","        img_path = str(Path(self.ds_path) / f'WIDER_{self.mode}' / 'images' / info['filename'])\n","        img = cv2.imread(img_path)  # BGR\n","\n","        if self.color_layout.lower() == 'rgb':\n","            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","\n","        assert img is not None, f'Image Not Found {img_path}'\n","        h0, w0 = img.shape[:2]  # orig hw\n","        return img, img_path, (h0, w0), idx  # img, hw_original, hw_resized\n","\n","    def __getitem__(self, index):\n","        image, fp, shape, img_id = self._load_image(index)\n","        bb_data, kp_data = self.annotations[index]\n","\n","        bb, bb_labels, bb_ignore, bb_ids = bb_data\n","        kp, kp_labels, kp_ignore, kp2bb_ids = kp_data\n","\n","        if self.transforms:\n","            # apply albumentations transform\n","            transformed = self.transforms(\n","                image=image,\n","                bboxes=bb,\n","                bb_classes=bb_labels,\n","                bb_ignore=bb_ignore,\n","                bb_id=bb_ids,\n","                keypoints=kp,\n","                kp_classes=kp_labels,\n","                kp_ignore=kp_ignore,\n","                kp2bb_id=kp2bb_ids,\n","            )\n","            image = transformed['image']\n","            bb, kp = transformed['bboxes'], transformed['keypoints']\n","            bb_labels, kp_labels = transformed['bb_classes'], transformed['kp_classes']\n","            bb_ignore, kp_ignore = transformed['bb_ignore'], transformed['kp_ignore']\n","            bb_ids = transformed['bb_id']\n","            kp2bb_ids = transformed['kp2bb_id']\n","\n","        bboxes = np.zeros((len(bb), 7), dtype=np.float32)  # (img_id, cat_id, weight, x1, y1, x2, y2)\n","        key_points = np.zeros((len(bb), 16), dtype=np.float32)  # (img_id, x1, y1, w1, ... x5, y5, w5)\n","\n","        if len(bb):\n","            bboxes[:, 1] = np.array(bb_labels)\n","            bboxes[:, 2] = np.array(bb_ignore) == 0.0\n","            bboxes[:, 3:] = xywh2xyxy(np.array(bb))\n","\n","            _id_map = {bb_ids[i]: i for i in range(len(bboxes))}\n","\n","            for i, (point, ignore, box_id) in enumerate(zip(kp, kp_ignore, kp2bb_ids)):\n","                lbl_id = i % 5\n","                weight = 1.0 if is_in_image(point, image.size()[-2:]) and not ignore else 0.0\n","                start_ind = 1 + 3 * lbl_id\n","                end_ind = 1 + 3 * (lbl_id + 1)\n","                box_id = _id_map.get(box_id, None)\n","\n","                if box_id is not None:\n","                    key_points[box_id, start_ind:end_ind] = [*point, weight]\n","\n","        bboxes = torch.tensor(bboxes, dtype=torch.float)\n","        key_points = torch.tensor(key_points, dtype=torch.float)\n","\n","        return image, bboxes, key_points, img_id, fp, shape\n","\n","    def _parse_ann_line(self, line):\n","        values = [float(x) for x in line.strip().split()]\n","        bbox = np.array(values[0:4], dtype=np.float32)\n","        kps = np.zeros((self.NK, 3), dtype=np.float32)\n","        ignore = False\n","        if self.min_size is not None:\n","            assert not self.test_mode\n","            w = bbox[2] - bbox[0]\n","            h = bbox[3] - bbox[1]\n","            if w < self.min_size or h < self.min_size:\n","                ignore = True\n","        if len(values) > 4:\n","            if len(values) > 5:\n","                kps = np.array(values[4:19], dtype=np.float32).reshape((self.NK, 3))\n","                for li in range(kps.shape[0]):\n","                    if (kps[li, :] == -1).all():\n","                        kps[li][2] = 0.0    # weight = 0, ignore\n","                    else:\n","                        assert kps[li][2] >= 0\n","                        kps[li][2] = 1.0    # weight\n","            else:\n","                if not ignore:\n","                    ignore = (values[4] == 1)\n","\n","        return dict(bbox=bbox, kps=kps, ignore=ignore, cat='Face')\n","\n","    def load_annotations(self, ann_file):\n","        \"\"\"Load annotation from COCO style annotation file.\n","\n","        Args:\n","            ann_file (str): Path of annotation file.\n","\n","        Returns:\n","            list[dict]: Annotation info from COCO api.\n","        \"\"\"\n","        name = None\n","        bbox_map = {}\n","        for line in open(ann_file, 'r'):\n","            line = line.strip()\n","            if line.startswith('#'):\n","                value = line[1:].strip().split()\n","                name = value[0]\n","                width = int(value[1])\n","                height = int(value[2])\n","\n","                bbox_map[name] = dict(width=width, height=height, objs=[])\n","                continue\n","\n","            assert name is not None\n","            assert name in bbox_map\n","            bbox_map[name]['objs'].append(line)\n","\n","        data_infos = []\n","        for name in bbox_map:\n","            item = bbox_map[name]\n","            width = item['width']\n","            height = item['height']\n","            vals = item['objs']\n","\n","            objs = []\n","            for line in vals:\n","                data = self._parse_ann_line(line)\n","                if data is None:\n","                    continue\n","                objs.append(data)   # data is (bbox, kps, cat)\n","\n","            # if len(objs) == 0:\n","            #     continue\n","\n","            data_infos.append(dict(filename=name, width=width, height=height, objs=objs))\n","\n","        out_ann = []\n","        images = []\n","\n","        for info in data_infos:\n","            objects = info['objs']\n","            images.append({k: info[k] for k in ['filename', 'width', 'height']})\n","\n","            n_anns = len(objects)\n","\n","            bb = np.zeros((n_anns, 4), dtype=np.float32)\n","            bb_labels = np.zeros((n_anns, 1), dtype=np.int32)\n","            bb_ignore = np.zeros((n_anns, 1), dtype=np.bool_)\n","            bb_ids = np.zeros((n_anns, 1), dtype=np.int32)\n","\n","            kp = np.zeros((n_anns, self.NK, 2), dtype=np.float32)\n","            kp_labels = np.zeros((n_anns, self.NK), dtype=np.int32)\n","            kp_ignore = np.zeros((n_anns, self.NK), dtype=np.bool_)\n","            kp_bb_ids = np.zeros((n_anns, self.NK), dtype=np.int32)\n","\n","            for idx, obj in enumerate(objects):\n","                bb[idx, :] = np.array(obj['bbox'], dtype=np.float32)\n","                bb_labels[idx, :] = self.bb_cat2id[obj['cat']]\n","                bb_ignore[idx, :] = False  # todo:\n","                bb_ids[idx, :] = idx\n","\n","                kp[idx, :, :] = obj['kps'][:, :2]\n","                kp_labels[idx, :] = [self.kp_cat2id[_] for _ in self.KP_CLASS_LABELS]\n","                kp_ignore[idx, :] = obj['kps'][:, 2] == 0\n","                kp_bb_ids[idx, :] = [idx for _ in self.KP_CLASS_LABELS]\n","\n","            bb = xyxy2xywh(bb)\n","\n","            kp[kp == -1] = 0    # replace -1 with 0\n","\n","            img_shape = (info['height'], info['width'])\n","\n","            bb = self.validate_bb(bb, img_shape)\n","            kp, ignore = self.validate_kp(kp, bb)\n","\n","            bb_data = [bb, bb_labels.flatten(), bb_ignore.flatten(), bb_ids.flatten()]\n","            kp_data = [kp, kp_labels, kp_ignore, kp_bb_ids]\n","\n","            # assert len(bb_data[0]) > 0\n","            # kp_data['kp'] = self.validate_kp(kp_data['kp'], bb_data['bb'])\n","\n","            kp_data[0] = kp_data[0].reshape(-1, 2)\n","            kp_data[1] = kp_data[1].flatten()\n","            kp_data[2] = kp_data[2].flatten()\n","            kp_data[3] = kp_data[3].flatten()\n","            out_ann.append((bb_data, kp_data))\n","\n","        return images, out_ann\n","\n","    def validate_bb(self, bb, img_shape):\n","        bb = xywh2xyxy(np.array(bb))\n","        h, w = img_shape\n","        bb[:, 0::2] = bb[:, 0::2].clip(0, w - 1)\n","        bb[:, 1::2] = bb[:, 1::2].clip(0, h - 1)\n","        bb = xyxy2xywh(bb)\n","        return bb\n","\n","    def validate_kp(self, kp, bb):\n","        \"\"\"\n","        Clip key points to the bbox size. If kp coords differ more than by 1 pixel: kp is marked as ignore\n","        bb: np.array(nl, 4)\n","        kp: np.array(nl, 5, 2)\n","        \"\"\"\n","        ignore_list = []\n","        for i in range(len(kp)):\n","            box = bb[i]\n","\n","            x1, y1, x2, y2 = box[0], box[1], box[0]+box[2], box[1]+box[3]\n","\n","            # set outbound kps as ignored\n","            x_ignore_1 = kp[i, :, 0] < x1 - 1\n","            x_ignore_2 = kp[i, :, 0] > x2\n","            y_ignore_1 = kp[i, :, 1] < y1 - 1\n","            y_ignore_2 = kp[i, :, 1] > y2\n","            ignore = np.sum(np.stack([x_ignore_1, x_ignore_2, y_ignore_1, y_ignore_2]), axis=0) > 0\n","\n","            # clip all key points to the bbox size\n","            kp[i, :, 0] = kp[i, :, 0].clip(x1, x2 - 1)\n","            kp[i, :, 1] = kp[i, :, 1].clip(y1, y2 - 1)\n","\n","            ignore_list.append(ignore)\n","        return kp, np.array(ignore_list)\n","\n","    @property\n","    def get_ds_name(self):\n","        return str(Path(self.ds_path).name)\n","\n","    @staticmethod\n","    def collate_fn(batch):\n","        return collate_fn(batch)"],"metadata":{"id":"5xnL-K4_mOou"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from src.upd_scrfd.scrfd_10g import SCRFD_10G\n","from src.upd_scrfd.scrfd import SCRFD_500M\n","from src.losses.reviewkd_loss import build_kd_trans\n","\n","class KDSCRFD_500M(SCRFD_500M):\n","    def __init__(self, nc, strides=(8, 16, 32), export=True):\n","        super().__init__(nc, strides, export)\n","        self.kd_trans = build_kd_trans(None)\n","\n","    def forward(self, x):\n","        device = x.device\n","        backbone_feats = self.backbone(x)\n","        neck_feat = self.neck(backbone_feats)\n","        anchors = self.anchor_generator(x, neck_feat, device)\n","        cls_scores, bboxes, key_points = self.bbox_head(neck_feat)\n","\n","        if self.training:\n","            neck_feat = self.kd_trans(neck_feat)\n","            return neck_feat, (cls_scores, bboxes, key_points, anchors)\n","        return cls_scores, bboxes, key_points, anchors\n","\n","class KDSCRFD_10G(SCRFD_10G):\n","    def forward(self, x):\n","        device = x.device\n","        backbone_feats = self.backbone(x)\n","        neck_feat = self.neck(backbone_feats)\n","        anchors = self.anchor_generator(x, neck_feat, device)\n","        cls_scores, bboxes, key_points = self.bbox_head(neck_feat)\n","\n","        if self.training:\n","            neck_feat = self.kd_trans(neck_feat)\n","            return neck_feat, (cls_scores, bboxes, key_points, anchors)\n","        return cls_scores, bboxes, key_points, anchors"],"metadata":{"id":"Xl8tHbvv7e4t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torchvision\n","import albumentations as A\n","import albumentations_experimental as AE\n","\n","from tqdm import tqdm\n","from loguru import logger\n","from albumentations.pytorch import ToTensorV2\n","from torchmetrics import MultioutputWrapper, MeanMetric\n","\n","from src.evaluator import DSMetrics, Metrics\n","from src.losses.reviewkd_loss import hcl\n","from src.evaluator import WiderFaceEvaluator\n","from src.transforms import RandomSmartCrop\n","from src.losses.detection_loss import MultiOutputDetectionLoss\n","from src.losses.qfl import QualityFocalLoss\n","from src.losses.iou_loss import DIoULoss\n","from src.losses.smooth_l1 import SmoothL1Loss\n","from src.upd_scrfd.utils import distance2kps, distance2bbox, kps2distance"],"metadata":{"id":"KEDavt1zZrMc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def setup_tqdm_loader(data_loader, mode, epoch, max_epochs, loader_ind=None):\n","    if mode == 'train':\n","        desc = f'Train'.ljust(10) + f'{epoch}/{max_epochs}'\n","    elif mode == 'val':\n","        if loader_ind is None:\n","            split = f'Val'.ljust(10)\n","        else:\n","            split = f'Val_{loader_ind}'.ljust(10)\n","        desc = split + f'{epoch}/{max_epochs}'\n","    else:\n","        raise NotImplementedError\n","\n","    bar_format = '{l_bar}{bar}{r_bar}'\n","    enabled = True\n","    pbar = tqdm(data_loader, disable=not enabled, bar_format=bar_format, total=len(data_loader), desc=desc)\n","    return pbar\n","\n","def get_anchor_centers(fm_h, fm_w, stride, na):\n","    xv, yv = torch.meshgrid(torch.arange(fm_w), torch.arange(fm_h), indexing='ij')\n","    anchor_centers = torch.stack([yv, xv], dim=-1).float()\n","    anchor_centers = (anchor_centers * stride).reshape((-1, 2))\n","    anchor_centers = torch.stack([anchor_centers] * na, dim=1).reshape((-1, 2))\n","    return anchor_centers\n","\n","\n","def postprocess(raw_pred, iou_thresh, conf_thresh):\n","        num_classes = 1\n","        strides = (8, 16, 32)\n","        cls_scores, bboxes, key_points, _ = raw_pred\n","\n","        bb_per_lvl = []\n","        kp_per_lvl = []\n","        scores_per_lvl = []\n","        labels_per_lvl = []\n","        per_lvl_batch_ind = []\n","\n","        for stride_idx, stride in enumerate(strides):\n","            bbox = bboxes[stride_idx]\n","            kps = key_points[stride_idx]\n","\n","            device = bbox.device\n","\n","            b, AxC, h, w = bbox.shape\n","            na = AxC // 4\n","            c = 4\n","            bbox = bbox.view(b, na, c, h, w)\n","            bbox = bbox.permute(0, 3, 4, 1, 2)\n","            bbox = bbox.reshape(-1, c)\n","            bbox *= stride\n","\n","            anchor_centers = get_anchor_centers(h, w, stride, na)\n","            anchor_centers = anchor_centers.repeat(b, 1)\n","            anchor_centers = anchor_centers.to(device)\n","\n","            bbox = distance2bbox(anchor_centers, bbox)\n","            bb_per_lvl.append(bbox)\n","\n","            b, AxC, h, w = kps.shape\n","            na = AxC // 10\n","            c = 10\n","            kps = kps.view(b, na, c, h, w)\n","            kps = kps.permute(0, 3, 4, 1, 2)\n","            kps = kps.reshape(-1, c)\n","            kps *= stride\n","\n","            kps = distance2kps(anchor_centers, kps)\n","            kp_per_lvl.append(kps)\n","\n","            scores = cls_scores[stride_idx]\n","            b, AxC, h, w = scores.shape\n","            na = AxC // num_classes\n","            c = num_classes\n","            scores = scores.view(b, na, c, h, w)\n","            scores = scores.permute(0, 3, 4, 1, 2)\n","            scores = scores.reshape(-1, c)\n","            scores = scores.sigmoid()\n","            scores, labels = torch.max(scores, dim=1)\n","\n","            scores_per_lvl.append(scores)\n","            labels_per_lvl.append(labels)\n","\n","            batch_ind = torch.arange(b, device=device).view(-1, 1).repeat(1, na * w * h).flatten()  # b x (na*w*h)\n","            per_lvl_batch_ind.append(batch_ind)\n","\n","        bboxes = torch.cat(bb_per_lvl)\n","        kps = torch.cat(kp_per_lvl)\n","        scores = torch.cat(scores_per_lvl)\n","        labels = torch.cat(labels_per_lvl)\n","        idxs = torch.cat(per_lvl_batch_ind)\n","\n","        # todo: returns tuple check?\n","        is_pos = torch.where(scores > conf_thresh)[0]\n","        bboxes = bboxes[is_pos]\n","        kps = kps[is_pos]\n","        scores = scores[is_pos]\n","        labels = labels[is_pos]\n","        idxs = idxs[is_pos]\n","\n","        keep_after_nms = torchvision.ops.batched_nms(bboxes, scores, idxs, iou_threshold=iou_thresh)\n","\n","        bboxes = bboxes[keep_after_nms]\n","        kps = kps[keep_after_nms]\n","        scores = scores[keep_after_nms]\n","        labels = labels[keep_after_nms]\n","        idxs = idxs[keep_after_nms]\n","\n","        out_bboxes = [[] for _ in range(b)]\n","        out_scores = [[] for _ in range(b)]\n","        out_labels = [[] for _ in range(b)]\n","        out_key_points = [[] for _ in range(b)]\n","        for bbox, kp, score, lbl, idx in zip(bboxes, kps, scores, labels, idxs):\n","            out_scores[idx].append(score)\n","            out_bboxes[idx].append(bbox)\n","            out_key_points[idx].append(kp)\n","            out_labels[idx].append(lbl)\n","        # cls_score, labels, bbox_pred, kps\n","        return out_scores, out_labels, out_bboxes, out_key_points\n","\n","\n","def get_transforms(mode):\n","    if mode == 'train':\n","        t = A.Compose([\n","                RandomSmartCrop(p=1),\n","                A.LongestMaxSize(max_size=640),\n","                A.PadIfNeeded(min_height=640, min_width=640, value=[0, 0, 0], border_mode=0, position='top_left'),\n","                A.ColorJitter(hue=0.0705, saturation=[0.5, 1.5], contrast=[0.5, 1.5], brightness=0.1254, p=0.3),\n","                AE.HorizontalFlipSymmetricKeypoints(symmetric_keypoints=[[0, 1], [2, 2], [3, 4]], p=0.5),\n","                A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.50196, 0.50196, 0.50196]),\n","                ToTensorV2(),\n","            ],\n","            bbox_params=A.BboxParams(format='coco', min_visibility=0.7, label_fields=['bb_classes', 'bb_ignore', 'bb_id']),\n","            keypoint_params=A.KeypointParams(format='xy', label_fields=[ 'kp_classes', 'kp2bb_id', 'kp_ignore' ], remove_invisible=False)\n","        )\n","    else:\n","        t = A.Compose([\n","            A.LongestMaxSize(max_size=640),\n","            A.PadIfNeeded(min_height=640, min_width=640, value=[0, 0, 0], border_mode=0, position='top_left'),\n","            A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.50196, 0.50196, 0.50196]),\n","            ToTensorV2(),\n","        ],\n","            bbox_params=A.BboxParams(format='coco', min_visibility=0.7,\n","                                     label_fields=['bb_classes', 'bb_ignore', 'bb_id']),\n","            keypoint_params=A.KeypointParams(format='xy', label_fields=['kp_classes', 'kp2bb_id', 'kp_ignore'],\n","                                             remove_invisible=True)\n","        )\n","    return t"],"metadata":{"id":"WtCNhcANbyQi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_epoch(model, dataloader, optimizer, lr_scheduler, criterion, epoch, max_epochs, device, debug=False):\n","    model.train()\n","    loss_avg = MultioutputWrapper(base_metric=MeanMetric(), num_outputs=criterion.num_loss_items).to(device)\n","\n","    pbar = setup_tqdm_loader(dataloader, mode='train', epoch=epoch, max_epochs=max_epochs)\n","\n","    # bb_targets: [N, 7]   (img_id, cat_id, ignore, x1, y1, x2, y2)\n","    # kp_targets: [N, 17]  (img_id, box_id, x1, y1, i1, ... x5, y5, i5)\n","    for iter, batch in enumerate(pbar):\n","        img = batch['image']\n","        img_shape = batch['image'].size()[-2:]\n","        targets = {'bb': batch['bb'].to(device), 'kp': batch['kp'].to(device)}\n","        img = img.to(device)\n","\n","        # todo:\n","        raw_output = model(img)\n","        loss, loss_items = criterion(raw_output, targets, img_shape)\n","\n","        if torch.any(torch.isnan(loss_items)):\n","            logger.warning('Nan Loss encountered')\n","        else:\n","            loss_avg.update(loss_items.unsqueeze(0))\n","            loss.backward()\n","\n","        # optimize\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # lr scheduler step (per iteration)\n","        if lr_scheduler:\n","            lr_scheduler.step()\n","\n","        # tqdm pbar postfix\n","        avg_losses = loss_avg.compute()\n","        loss_names = criterion.loss_items_names\n","        avg_losses = {name: f'{value:.4f}' for name, value in zip(loss_names, avg_losses)}\n","        mem = (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n","        pbar.set_postfix({**avg_losses, 'gpu_mem': f'{mem:.2f}Gb', 'img_size': list(img_shape)})\n","\n","        if debug and iter > 5:\n","            break\n","\n","    loss_items = map(float, loss_avg.compute())\n","    loss_items = dict(zip(criterion.loss_items_names, loss_items))\n","    train_metrics = DSMetrics(losses=loss_items)\n","    train_metrics = Metrics(metrics=[train_metrics], split_names=['Train'])\n","    return model, optimizer, lr_scheduler, train_metrics"],"metadata":{"id":"Q9-wisHCf92P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def nd_loss(ND_loss, s_emb, t_emb, T_EMB, labels):\n","    nd = ND_loss(s_emb, t_emb, T_EMB, labels)\n","    return {\n","        'loss_nd': nd,\n","    }\n","\n","\n","def distillation_train_epoch(s_model, t_model, dataloader, optimizer, lr_scheduler, criterion, epoch, max_epochs, device, debug=False):\n","    s_model.train()\n","    loss_avg = MultioutputWrapper(base_metric=MeanMetric(), num_outputs=criterion.num_loss_items).to(device)\n","\n","    pbar = setup_tqdm_loader(dataloader, mode='train', epoch=epoch, max_epochs=max_epochs)\n","\n","    # T_EMB = ...\n","    # T = ...\n","\n","    # ND_loss = DirectNormLoss(num_class=1, nd_weight=1.5)\n","    # KD_loss = KDLoss(kd_weight=2.2, T=T)\n","\n","    # bb_targets: [N, 7]   (img_id, cat_id, ignore, x1, y1, x2, y2)\n","    # kp_targets: [N, 17]  (img_id, box_id, x1, y1, i1, ... x5, y5, i5)\n","    for iter, batch in enumerate(pbar):\n","        img = batch['image']\n","        img_shape = batch['image'].size()[-2:]\n","        targets = {'bb': batch['bb'].to(device), 'kp': batch['kp'].to(device)}\n","        img = img.to(device)\n","\n","        s_features, s_raw_output = s_model(img)\n","\n","        with torch.no_grad():\n","            t_features, t_raw_output = t_model(img)\n","\n","        # compute loss\n","        det_loss, loss_items = criterion(s_raw_output, targets, img_shape)\n","\n","        # ND Loss\n","        # gt = [x.gt_classes for x in sampled_proposals]\n","        # gt_classes = torch.cat(tuple(gt), 0).reshape(-1)\n","        # nd_loss_dict = nd_loss(ND_loss=ND_loss, s_emb=s_emb, t_emb=t_emb, T_EMB=T_EMB, labels=gt_classes)\n","        # reviewkd loss\n","        # t_features = [t_features[f] for f in t_features]\n","        # s_features = [s_features[f] for f in s_features]\n","        reviewkd_loss = hcl(s_features, t_features) * 2.0\n","\n","        loss = det_loss + reviewkd_loss\n","\n","\n","        if torch.any(torch.isnan(loss_items)):\n","            logger.warning('Nan Loss encountered')\n","        else:\n","            loss_avg.update(loss_items.unsqueeze(0))\n","            loss.backward()\n","\n","        # optimize\n","        optimizer.step()\n","        optimizer.zero_grad()\n","\n","        # lr scheduler step (per iteration)\n","        if lr_scheduler:\n","            lr_scheduler.step()\n","\n","        # tqdm pbar postfix\n","        avg_losses = loss_avg.compute()\n","        loss_names = criterion.loss_items_names\n","        avg_losses = {name: f'{value:.4f}' for name, value in zip(loss_names, avg_losses)}\n","        mem = (torch.cuda.memory_reserved() / 1E9 if torch.cuda.is_available() else 0)  # (GB)\n","        pbar.set_postfix({**avg_losses, 'gpu_mem': f'{mem:.2f}Gb', 'img_size': list(img_shape)})\n","\n","        if debug and iter > 5:\n","            break\n","\n","    loss_items = map(float, loss_avg.compute())\n","    loss_items = dict(zip(criterion.loss_items_names, loss_items))\n","    train_metrics = DSMetrics(losses=loss_items)\n","    train_metrics = Metrics(metrics=[train_metrics], split_names=['Train'])\n","    return s_model, optimizer, lr_scheduler, train_metrics"],"metadata":{"id":"NLrxrI3OgET1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import time\n","def eval(model, dataloader, postprocess, device='cpu', debug=False):\n","    model.eval()\n","    desc = f'Evaluating'\n","    bar_format = '{l_bar}{bar}{r_bar}'\n","    pbar = tqdm(dataloader, bar_format=bar_format, total=len(dataloader), desc=desc)\n","\n","    gt_dir = 'data/widerface/WIDER_val/gt'\n","    evaluator = WiderFaceEvaluator(gt_dir, iou_thresh=0.5)\n","\n","    # time_arr = []\n","    with torch.no_grad():\n","        for i, batch in enumerate(pbar):\n","            img = batch['image']\n","            img_shape = batch['image'].size()[-2:]\n","            targets = {'bb': batch['bb'].to(device), 'kp': batch['kp'].to(device)}\n","            img = img.to(device)\n","\n","            t0 = time.time()\n","            raw_output = model(img)\n","            output = postprocess(raw_output)\n","            t1 = time.time()\n","\n","            meta_data = {'img_id': batch['img_id'], 'img0_shape': batch['img_shape'], 'img1_shape': img_shape,\n","                         'img_path': batch['img_path']}\n","\n","            evaluator.add_batch(output, targets, meta_data)\n","\n","            # if i > 10:\n","            #     time_arr.append(t1-t0)\n","\n","            if debug and i > 5:\n","                break\n","\n","        metrics = evaluator.compute()\n","        # print(sum(time_arr) / len(time_arr) * 1000)\n","    return metrics"],"metadata":{"id":"clkLVspGfna9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["debug = False\n","lr = 0.003\n","max_epochs = 30\n","device = 'cuda:0'\n","ds_path = 'data/widerface'\n","\n","# train split\n","t_transforms = get_transforms('train')\n","t_dataset = WiderFaceDataset(ds_path, 'train', min_size=None, transforms=t_transforms, color_layout='RGB')\n","train_dataloader = torch.utils.data.DataLoader(t_dataset, batch_size=16, num_workers=2, pin_memory=True, collate_fn=t_dataset.collate_fn)\n","\n","# val split\n","v_transforms = get_transforms('val')\n","v_dataset = WiderFaceDataset(ds_path, 'val', min_size=None, transforms=v_transforms, color_layout='RGB')\n","val_dataloader = torch.utils.data.DataLoader(v_dataset, batch_size=16, num_workers=2, pin_memory=True, collate_fn=v_dataset.collate_fn)\n","\n","postproc = lambda x: postprocess(x, conf_thresh=0.02, iou_thresh=0.45)\n","\n","# init teacher model\n","t_model = KDSCRFD_10G(nc=1).to(device)\n","t_model.load_from_checkpoint('weights/upd_SCRFD_10G_KPS.pth')\n","\n","# init student model\n","s_model = KDSCRFD_500M(nc=1).to(device)\n","s_model.load_from_checkpoint('weights/upd_SCRFD_500M_KPS.pth', strict=False)\n","\n","# init optimizer\n","p_groups = s_model.get_param_groups(wd=1e-4, no_decay_bn_filter_bias=True)\n","optimizer = torch.optim.Adam(p_groups, lr=lr)\n","\n","# lr_scheduler\n","lr_scheduler = None\n","\n","# loss\n","cls_loss = QualityFocalLoss(use_sigmoid=True, beta=2.0, loss_weight=1.0)\n","bb_loss = DIoULoss(loss_weight=2.0)\n","kp_loss = SmoothL1Loss(beta=0.1111111111111111, loss_weight=0.1)\n","criterion = MultiOutputDetectionLoss(cls_loss=cls_loss, bb_loss=bb_loss, kp_loss=kp_loss)\n","\n","best_score = ((0.9071 + 0.8805 + 0.6768) / 3, -1)\n","for i in range(max_epochs):\n","    out = distillation_train_epoch(\n","        s_model,\n","        t_model,\n","        train_dataloader,\n","        optimizer,\n","        lr_scheduler,\n","        criterion,\n","        epoch=i,\n","        max_epochs=max_epochs,\n","        device=device,\n","        debug=debug\n","    )\n","\n","    metrics = eval(s_model, val_dataloader, postproc, debug=False, device=device)\n","    score = metrics.get_fitness_score(labels=['bb_easy_AP', 'bb_medium_AP', 'bb_hard_AP'], weights=[1.0, 1.0, 1.0])\n","    print(f'Current score: {score}, best_score: {best_score[0]}')\n","    print(f'Metrics: {metrics}')\n","\n","    best_score = (max(score, best_score[0]), i)\n","    ckpt = {\n","        'model': s_model.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","        'epoch': i,\n","        'metrics': metrics,\n","    }\n","    torch.save(ckpt, f'runs1/reviewkd_ckpt_{i}_{score:.4f}.pt')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGOE40ZlgN67","outputId":"e205978f-7fc5-44d3-8ce4-8cb25d8f5abb"},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Train     0/30: 100%|██████████| 805/805 [09:18<00:00,  1.44it/s, cls_QFL=0.1288, bb_DIoU=0.2465, kp_SmoothL1=0.3094, Total=0.2282, gpu_mem=7.01Gb, img_size=[640, 640]]\n","Evaluating: 100%|██████████| 202/202 [13:53<00:00,  4.13s/it]\n","/content/src/evaluator.py:441: RuntimeWarning: invalid value encountered in double_scalars\n","  _pr_curve[i, 0] = pr_curve[i, 1] / pr_curve[i, 0]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Current score: 0.3428982120005228, best_score: 0.8214666666666667\n","Metrics: Detection Metrics:\n","easy_AP: 0.4760, medium_AP: 0.3795, hard_AP: 0.1731\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Train     1/30: 100%|██████████| 805/805 [09:11<00:00,  1.46it/s, cls_QFL=0.1288, bb_DIoU=0.2394, kp_SmoothL1=0.2934, Total=0.2206, gpu_mem=7.01Gb, img_size=[640, 640]]\n","Evaluating: 100%|██████████| 202/202 [08:39<00:00,  2.57s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Current score: 0.370182983358564, best_score: 0.8214666666666667\n","Metrics: Detection Metrics:\n","easy_AP: 0.5268, medium_AP: 0.4032, hard_AP: 0.1805\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Train     2/30: 100%|██████████| 805/805 [09:10<00:00,  1.46it/s, cls_QFL=0.1286, bb_DIoU=0.2349, kp_SmoothL1=0.2776, Total=0.2137, gpu_mem=7.01Gb, img_size=[640, 640]]\n","Evaluating: 100%|██████████| 202/202 [10:06<00:00,  3.00s/it]\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Current score: 0.3242337038640175, best_score: 0.8214666666666667\n","Metrics: Detection Metrics:\n","easy_AP: 0.4469, medium_AP: 0.3627, hard_AP: 0.1632\n"]},{"output_type":"stream","name":"stderr","text":["Train     3/30: 100%|██████████| 805/805 [09:08<00:00,  1.47it/s, cls_QFL=0.1265, bb_DIoU=0.2405, kp_SmoothL1=0.2925, Total=0.2198, gpu_mem=7.01Gb, img_size=[640, 640]]\n","Evaluating: 100%|██████████| 202/202 [06:25<00:00,  1.91s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Current score: 0.47637578063184577, best_score: 0.8214666666666667\n","Metrics: Detection Metrics:\n","easy_AP: 0.6336, medium_AP: 0.5385, hard_AP: 0.2571\n"]},{"output_type":"stream","name":"stderr","text":["Train     4/30:  80%|████████  | 648/805 [07:27<01:40,  1.57it/s, cls_QFL=0.1264, bb_DIoU=0.2364, kp_SmoothL1=0.2762, Total=0.2130, gpu_mem=7.01Gb, img_size=[640, 640]]"]}]},{"cell_type":"code","source":["score = metrics.get_fitness_score(labels=['bb_easy_AP', 'bb_medium_AP', 'bb_hard_AP'], weights=[1.0, 1.0, 1.0])\n","\n","# easy_AP: 0.9071, medium_AP: 0.8805, hard_AP: 0.6768"],"metadata":{"id":"xR664YXEt0YW"},"execution_count":null,"outputs":[]}]}